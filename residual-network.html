
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="robots" content="" />

  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro|Source+Sans+Pro:300,400,400i,700" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="/theme/stylesheet/style.min.css">

  <link rel="stylesheet" type="text/css" href="/theme/pygments/github.min.css">
  <link rel="stylesheet" type="text/css" href="/theme/font-awesome/css/font-awesome.min.css">

    <link href="/static/custom.css" rel="stylesheet">






<meta name="author" content="Zhuqian" />
<meta name="description" content="General How to analysis neural network abilities From two aspects: Representation Abilities. NN == RN Optimization. RN is easier to optimize Residual network has same representation ablitiy with tranditional neual netork. Proposed residual network in Identity_Mappings_in_Deep_Residual_Networks has less representional ability than gating and 1x1 convolutional shortcuts. But it get better experimental …" />
<meta name="keywords" content="">

<meta property="og:site_name" content="Financer Knowledge Graph"/>
<meta property="og:title" content="Residual Network"/>
<meta property="og:description" content="General How to analysis neural network abilities From two aspects: Representation Abilities. NN == RN Optimization. RN is easier to optimize Residual network has same representation ablitiy with tranditional neual netork. Proposed residual network in Identity_Mappings_in_Deep_Residual_Networks has less representional ability than gating and 1x1 convolutional shortcuts. But it get better experimental …"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="/residual-network.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2019-03-18 16:00:00+08:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="/author/zhuqian.html">
<meta property="article:section" content="misc"/>
<meta property="og:image" content="">

  <title>Financer Knowledge Graph &ndash; Residual Network</title>

</head>
<body>
  <aside>
    <div>
      <a href="">
        <img src="/theme/img/profile.png" alt="" title="">
      </a>
      <h1><a href=""></a></h1>


      <nav>
        <ul class="list">

          <li><a href="http://getpelican.com/" target="_blank">Pelican</a></li>
          <li><a href="http://python.org/" target="_blank">Python.org</a></li>
          <li><a href="http://jinja.pocoo.org/" target="_blank">Jinja2</a></li>
          <li><a href="#" target="_blank">You can modify those links in your config file</a></li>
        </ul>
      </nav>

      <ul class="social">
        <li><a class="sc-You can add links in your config file" href="#" target="_blank"><i class="fa fa-You can add links in your config file"></i></a></li>
        <li><a class="sc-Another social link" href="#" target="_blank"><i class="fa fa-Another social link"></i></a></li>
      </ul>
    </div>


  </aside>
  <main>


<article class="single">
  <header>
      
    <h1 id="residual-network">Residual Network</h1>
    <p>
          Posted on 一 18 三月 2019 in <a href="/category/misc.html">misc</a>


    </p>
  </header>


  <div>
    <h1>General</h1>
<h2>How to analysis neural network abilities</h2>
<p>From two aspects: </p>
<ol>
<li>Representation Abilities. NN == RN</li>
<li>Optimization. RN is easier to optimize</li>
</ol>
<p>Residual network has same representation ablitiy with tranditional neual netork.</p>
<p>Proposed residual network in 
<a href="../media/Identity_Mappings_in_Deep_Residual_Networks_95817.pdf">Identity_Mappings_in_Deep_Residual_Networks</a> 
has less representional ability than gating and 1x1 convolutional shortcuts. But it get better experimental result. We will discuss the formula below.</p>
<h2>Problem on traditional NN</h2>
<ol>
<li>Deeper neural networks are more difficult(Accuracy/Efficiency) to train.</li>
<li>With the network depth increasing, accuracy gets saturated and then degrades rapidly(vanishing gradient).</li>
</ol>
<h1>Detail</h1>
<h2>Model Architecture</h2>
<table>
<thead>
<tr>
<th>Original in <a href="../media/Deep_residual_learning_for_image_recognition_c2a0b.pdf">Deep_residual_learning_for_image_recognition.pdf</a></th>
<th>Proposed in <a href="../media/Identity_Mappings_in_Deep_Residual_Networks_95817.pdf">Identity_Mappings_in_Deep_Residual_Networks.pdf</a></th>
</tr>
</thead>
<tbody>
<tr>
<td><img alt="image" src="/media/original_image.png"></td>
<td><img alt="image" src="/media/full_pre.png"></td>
</tr>
<tr>
<td>
<div class="math">$$ y_{1} = x_{l} + \mathcal F(x_{l}, \mathcal W_{l}) $$</div>
<div class="math">$$ x_{l+1} = relu(y_{1}) $$</div>
</td>
<td>
<div class="math">$$ y_{1} = x_{l} + \mathcal F(x_{l}, \mathcal W_{l}) $$</div>
<div class="math">$$ x_{l+1} = y_{1} $$</div>
</td>
</tr>
</tbody>
</table>
<h3>Formula</h3>
<ul>
<li><span class="math">\(x_{l}\)</span> and <span class="math">\(x_{l}+1\)</span> are input and output of the l-th unit, </li>
<li><span class="math">\(\mathcal F\)</span> is a residual function. Different residual network compose of various type of residual function. </li>
</ul>
<h2>Compare with tranditional NN</h2>
<table>
<thead>
<tr>
<th>\</th>
<th>Residual Network</th>
<th>Tranditional NN</th>
</tr>
</thead>
<tbody>
<tr>
<td>Unraveled View</td>
<td><img alt="image" src="/media/residual network.png"></td>
<td><img alt="image" src="/media/tranditional NN.png"></td>
</tr>
<tr>
<td>Formula</td>
<td>
<div class="math">\begin{aligned} y_{3} &amp;= y_{2} + f_{3}(y_{2})\\ &amp;=[y_{1} + f_{2}(y_{1})] + f_{3}(y_{1} + f_{2}(y_{1}))\\ &amp;=[y_{0} + f_{1}(y_{0}) + f_{2}(y_{0}+ f_{1}(y_{0}))] + f_{3}(y_{0} + f_{1}(y_{0}) + f_{2}(y_{0}+f_{1}(y_{0}))) \end{aligned}</div>
</td>
<td><span class="math">\(y_{3}^{FF} = f_{3}^{FF}(f_{2}^{FF}(f_{1}^{FF}(y_{0})))\)</span></td>
</tr>
<tr>
<td>Feature</td>
<td>have <strong>O(2n)</strong> implicit paths connecting input and output</td>
<td><strong>ONE</strong> path</td>
</tr>
</tbody>
</table>
<h2>More important: Gradient</h2>
<p>From <span class="math">\(x_{l+1} = x_{l} + \mathcal F(x_{l}, \mathcal W_{l})\)</span></p>
<p>To <span class="math">\(x_{L} = x_{l} + \sum_{i=l}^{L-1} \mathcal F(x_{i}, \mathcal W_{i})\)</span></p>
<ul>
<li>The model is in a <strong>residual</strong> fashion between any units L and l. (where the name residual come from)</li>
<li>The formula is summation instead of matrix-vector products</li>
</ul>
<p><strong>Gradient</strong>: </p>
<div class="math">$$\frac {\partial \varepsilon}{\partial x_{l}} = \frac {\partial \varepsilon}{\partial x_{L}}\frac{\partial x_{L}}{\partial x_{l}} = \frac{\partial \varepsilon}{\partial x_{L}}(1+\frac{\partial}{\partial x_{l}} \sum_{i=l}^{L-1} \mathcal F(x_{i}, \mathcal W_{i}))$$</div>
<ul>
<li>Information is directly propagated back to any shallower unit l <span class="math">\(\frac {\partial \varepsilon}{\partial x_{L}}\)</span></li>
<li>General the <span class="math">\(\frac{\partial}{\partial x_{l}} \sum_{i=l}^{L-1} \mathcal F\)</span> cannot be always -1</li>
</ul>
<p>So, the gradient of a layer <strong>does not</strong> vanish even when the weights are arbitrarily small.</p>
<h2>Various types of shortcut connections</h2>
<p><img alt="image" src="/media/shortcut connections.png"></p>
<h3>A simple modification</h3>
<div class="math">$$ x_{l+1} = \lambda_{l}x_{l} + \mathcal F(x_{l}, \mathcal W_{l})$$</div>
<div class="math">$$ x_{L} = (\prod_{i=l}^{L-1}\lambda_{i})x_{l} + \sum_{i=l}^{L-1}\hat{\mathcal F}(x_{i}, \mathcal W_{i})$$</div>
<div class="math">$$ \frac {\partial \varepsilon}{\partial x_{l}} = \frac {\partial \varepsilon}{\partial x_{L}}((\prod_{i=l}^{L-1}\lambda_{i})+\frac{\partial}{\partial x_{l}}\sum_{i=L}^{L-1} \hat{\mathcal F}(x_{i}, \mathcal W_{i}))$$</div>
<h2>Various types of activation function</h2>
<p><img alt="image" src="/media/activation function.png"></p>
<ul>
<li>Short out completely</li>
<li>BN for ease overfit</li>
</ul>
<h1>Reference</h1>
<ul>
<li><a href="/media/Deep_residual_learning_for_image_recognition_c2a0b.pdf">Deep_residual_learning_for_image_recognition_c2a0b.pdf</a></li>
<li><a href="/media/Identity_Mappings_in_Deep_Residual_Networks_95817.pdf">Identity_Mappings_in_Deep_Residual_Networks_95817.pdf</a></li>
<li><a href="/media/Residual_Networks_Behave_Like_Ensembles_of_Relatively_Shallow_Networks_13899.pdf">Residual_Networks_Behave_Like_Ensembles_of_Relatively_Shallow_Networks_13899.pdf</a></li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
  </div>
  <div class="tag-cloud">
    <p>
    </p>
  </div>





</article>

    <footer>
<p>&copy;  </p>
<p>    Powered by <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a>
</p>    </footer>
  </main>




<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Financer Knowledge Graph ",
  "url" : "",
  "image": "",
  "description": ""
}
</script>

</body>
</html>